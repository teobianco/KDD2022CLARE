# DynCLARE 

This repository contains the source code of DynCLARE method. It is an extended version of CLARE method presented in paper [CLARE: A Semi-supervised Community Detection Algorithm](https://dl.acm.org/doi/10.1145/3534678.3539370_). This source code is strongly based on CLARE method source code from repository https://github.com/FDUDSDE/KDD2022CLARE.

CLARE is an algorithm performing semi-supervised community detection and we extended it in two main ways:
- we made it able to work with **dynamic graphs** both in a naive and in a more structured way
- we added the possibility to exploit embeddings generated with other dynamic graph learning algorithms inside the *Community Locator*


## Table of Contents

- [DynCLARE](#DynCLARE)
  - [Table of Contents](#table-of-contents)
  - [Main Innovations](#main-innovations)
    - [Dynamic Graphs](#dynamic-graphs)
    - [Other Embedding Methods](#other-embedding-methods)
  - [Run CLARE](#run-clare) 
    - [Environmental Requirement](#environmental-requirement)
    - [Run the code](#run-the-code)


 ## Main Innovations

As already said there are 2 main innovations of DynCLARE w.r.t. CLARE: **Dynamic Graphs** applicability and possibility to **use other embedding methods**


### Dynamic Graphs

Differently from CLARE, which is static, DynCLARE does semi-supervised community detection on **dynamic graphs**. It can perform this task in two ways:
- by simply performing CLARE algorithm in each timestep separately (**naive** way)
- by performing CLARE algorithm in each timestep separately, but inizializing RL learnable weigths to those of previous timestep, thus preserving memory of previous communities structures (**structured** way)


### Other Embedding Methods

Original CLARE method exploits **order embedding** to provide community vector reperesentations to the *Community Locator*. Despite its good result, we think that adding the possibility to use embeddings generated with other methods can expand the fields in which this algorithm has high-quality performances

Moreover, in the context of this master's thesis, this feature allows to use embedding generated with other methods that are already able to manage dynamic graphs. Therefore, this let the temporal dimension of dynamic graphs get inside the algorithm not only in the *Community Rewriter* part (inside the RL, as described in previous section), but also inside the *Community Locator*. In this way we make DynCLARE an algorithm much more oriented towards semi-supervised dynamic community detection.


## Run CLARE


This repository contains the following contents:

```
.
├── Locator                       --> (The folder containing Community Locator source code)
├── Rewriter                      --> (The folder containing Community Rewriter source code)
├── ckpts                         --> (The folder saving checkpoint files)
├── dataset                       --> (The folder containing used datasets)
├── main.py                       --> (The main code file. The code is run through this file)
├── dataset_ONMI.py               --> (The file containing code to evaluate datasets communities stability through ONMI)
└── utils                         --> (The folder containing utils functions)


```
You have to create a `ckpts` folder to save contents.

For our experimental datasets, we put all three synthetic networks and just one semi-synthetic as example. All details concerning their generation can be found in the thesis.
Each dataset folder contains a folder called `time_{timestep}` for every timestep of the dynamic graph. Inside each of these folder there are a community file `{name}_{timstep}-1.90.cmty.txt` and an edge file `{name}_{timestep}-1.90.ungraph.txt`. Moreover, if the subdivision in train and test set is already done, in every `time_{timestep}` folder there are also a `train_set.txt` and a `test_set.txt` files. Each line of these files represents a community and contains ids of all nodes belonging to that community. If the train and test set are not supplied, they can be randomly generated by DynCLARE algorithm but **are not saved**

In order to use external embeddings with DynCLARE, each dataset folder must also contain a folder called `embedding` and a folder called `communities`. The first one contains folders named as the methods used to generate embeddings (e.g. `CTGCN-C`). Inside each '{method}' folder there must be a file for each timestep of the dynamic graph, called `graph{timestep}.csv` containing a DataFrame of node embeddings. As for `communities` folder, it must contain a file for each timestep of the dynamic graph called `communities{timestep}.txt` containing the same information as `{name}_{timstep}-1.90.cmty.txt` but in the format *node_id community_id*.

If you want to run on your **own datasets**, you have to convert your own data into our format, *i.e.*, **a community file** for every timestep where each line contains a unique community and **an edge file** for every timestep where each line contains an edge. All other folders and files are optional and have to be in the format described above.


### Environmental Requirement

0. You need to set up the environment for running the experiments (Python 3.7 or above)

1. Install **Pytorch** with version 1.8.0 or later

2.  Install **torch-geometric** package with version 2.0.1

    Note that it may need to appropriately install the package `torch-geometric` based on the CUDA version (or CPU version if GPU is not available). Please refer to the official website https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for more information of installing prerequisites.


### Run the code

Execute the `main.py` file

```
python main.py --dataset=Amazon_setting1  
```

Main arguments already in CLARE (for more argument options, please refer to `main.py`):

```
--dataset: the dataset to run
--num_pred / num_train / num_val: the numbers for prediction, training, and validation
--locator_epoch: number of epochs to train Community Locator (default setting 30)
--n_layers: ego-net dimensions & number of GNN layers (default 2)
--agent_lr: the learning rate of Community Rewriter
--max_step: the maximum operations (EXPAND/EXCLUDE) of rewriting a community
```

Arguments we added in DynCLARE

```
--already_train_test: a boolean to tell if the subdivision of communities in train and test set has already been done (default True)
--multiplier: works only if num_pred < num_train. DynCLARE looks for a number of communities equal to num_train * multiplier (default 1.0)
--memory: a boolean that enables the memory mechanism in RL if True (default False)
--method: a string to tell DynCLARE which method embeddings to use. If 'CLARE', it generates embedding as does CLARE algorithm. If '{method}', DynCLARE looks for a folder named '{method}' inside 'dataset/{dataset_name}/embedding' containing nodes embeddings for all timesteps (default 'CLARE')
```



  
